<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Sync-NeRF</title>

    <meta name="description" content="Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://bakedsdf.github.io/img/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://seoha-kim.github.io/sync-nerf"/>
    <meta property="og:title" content="Sync-NeRF" />
    <meta property="og:description" content="Project page for Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Sync-NeRF" />
    <meta name="twitter:description" content="Project page for Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos." />
    <meta name="twitter:image" content="https://seoha-kim.github.io/img/teaser.png"/>


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üç∏</text></svg>">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>

	<!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script> -->

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	
	<link rel="stylesheet" href="css/dics.min.css">
    <script src="scripts/dics.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', domReady);
        function domReady() {
            for (const e of document.querySelectorAll(".b-dics")) {
                new Dics({
                    container: e,
                    textPosition: "top"
                });
            }
        }
    </script>
	
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Sync-NeRF</b>: Generalizing Dynamic NeRFs 
                <br> to Unsynchronized Videos <br>
                <small>
                    ArXiv
                </small>
            </h2>
        </div>
		
		<div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <b>
                        Seoha Kim*
                        </b>
                        <br>Yonsei University
                    </li>
					<li>
                        <b>
                        Jeongmin Bae*
                        </b>
                        <br>Yonsei University
                    </li>
					<li>
                        <b>
                        Youngsik Yun
                        </b>
                        <br>Yonsei University
                    </li><br>
                    <li>
                        <b>
                        Hahyun Lee
                        </b>
                        <br>ETRI
                    </li>
					<li>
                        <b>
                        Gun Bang
                        </b>
                        <br>ETRI
                    </li>
                    <li>
                        <b>
                        Youngjung Uh
                        </b>
                        <br>Yonsei University
                    </li> 				
                </ul>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-12 text-center">
                *Denotes equal contribution <br>
                <a href="https://vilab.yonsei.ac.kr/">Visual Intelligence Lab in Yonsei University</a><br>
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="Sync_NeRF_Generalizing_Dynamic_NeRFs_to_Unsynchronized_Videos.pdf">
                            <image src="img/syncnerf_paper_image.png" height="120px"></image>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/seoha-kim/Sync-NeRF" target="_blank">
                            <image src="img/github.png" height="120px"></image>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://yonsei-my.sharepoint.com/personal/yj_uh_o365_yonsei_ac_kr/_layouts/15/onedrive.aspx?FolderCTID=0x012000911C4E8CA673AA4EA8B6758C572327B2&id=%2Fpersonal%2Fyj_uh_o365_yonsei_ac_kr%2FDocuments%2Fvilab-drive%2FSync-NeRF">
                            <image src="img/dataset_fox.png" height="120px"></image>
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Teaser
                </h3>
                <video poster="" id="teaser" autoplay controls muted loop playsinline width="100%;">
                    <source src="videos/teaser.mp4"
                            type="video/mp4">
                </video><br>
                <p class="text-justify">
                    The commonly used Plenoptic Video Dataset in 4D scene reconstruction contains an unsynchronized video. If we include this view in the training set, baselines fail to reconstruct the motion around the unsynchronized viewpoint. In the same settings, our method significantly outperforms.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <!-- <image src="img/rays.jpg" class="img-responsive" alt="overview"><br> -->
                <p class="text-justify">
                    Recent advancements in 4D scene reconstruction using neural radiance fields (NeRF) have demonstrated the ability to represent dynamic scenes from multi-view videos. However, they fail to reconstruct the dynamic scenes and struggle to fit even the training views in unsynchronized settings. It happens because they employ a single latent embedding for a frame while the multi-view images at the frame were actually captured at different moments. To address this limitation, we introduce time offsets for individual unsynchronized videos and jointly optimize the offsets with NeRF. By design, our method is applicable for various baselines and improves them with large margins. Furthermore, finding the offsets naturally works as synchronizing the videos without manual effort. Experiments are conducted on the common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to verify the performance and robustness of our method. The source code and the dataset will be released online.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <video poster="" id="results" autoplay controls muted playsinline width="100%;">
                    <source src="videos/results.mp4"
                            type="video/mp4">
                </video><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method Overview
                </h3>
                <h4>
                    Problem Statement
                </h4>
                <br>
                <image src="img/problem.png" style="width:100%;" alt="problem statement"></image>
                <br>
                <p class="text-justify">
                    <b>(a)</b> Ideally, all multi-view images at a frame captures the same moment of a scene. Each frame is represented by a latent embedding. 
                    <b>(b)</b> Some frames are not synchronized. Previous methods suffer from the discrepancy between the latent embedding of the frame and the actual status of the scene. 
                    <b>(c)</b> Our method allows assigning correct temporal latent embeddings to videos captured with temporal gaps by introducing learnable time offsets $\delta$ for individual cameras.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    Continuous temporal embedding
                </h4>
                <br>
                <image src="img/embedding.png" style="width:100%;" alt="continuous temporal embedding"></image>
                <br>
                <p class="text-justify">
                    <b>(a)</b> We present an implicit function-based approach for the methods utilizing per-frame temporal embeddings. We add time offset $\theta_{k}$ of camera $k$ to time input $t$. $T_{Œ∏}$ is the implicit function for mapping calibrated time into temporal embedding $z$. 
                    <b>(b)</b> We query the embedding at the calibrated time $t_{k}$ on grid-based models. Bilinear interpolation naturally allows continuous temporal embedding.
                </p>
            </div>
        </div>
	
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://bakedsdf.github.io/"><u>BakedSDF</u></a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
